{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">The missing values in a dataset refer to the absence of value in a particular observation for one or more feature on variable.This occur due for various reasons,such as incomplete data collection,data entry error or data correuption during transmission.\nHandling missing data ia important because they can cause bias in a analysis and inacurate conclusion.The presence of missing value are reduce the sample size,which as affect the power of statistical test and decrease the precision estimate. Most ML algorithm cannot handle missing value and may be return the error or produce incorrect result.There fore it is important to handle missing values. \nhere are some algorithms that not affect the missing values.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">i]Decision tree:It is type of supervised learning algorithm that can handle missing values by simply by ignoring them during the splitting precess.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">ii]Random forests: Random forests are an ensemble learning method that uses decision trees. They can handle missing values by imputing them with the median or mode of the feature.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">iii]K-nearest neighbors: K-nearest neighbors is a non-parametric algorithm that does not make any assumptions about the distribution of the data. It can handle missing values by imputing them with the mean or median of the feature.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">iv]Support vector machines: Support vector machines  are a type of supervised learning algorithm that can handle missing values by ignoring them or imputing them with the mean or median of the feature.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "> v]Naive Bayes: Naive Bayes is a probabilistic algorithm that assumes independence between the features. It can handle missing values by simply ignoring them during the training and testing phases.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q2: List down techniques used to handle missing data. Give an example of each with python code.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">i]Dropping missing value: This technique involves removing rows or columns that contain missing data.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport seaborn as sns\ndf=sns.load_dataset('titanic')\ndf.dropna()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ">ii]Imputing missing value:This technique involves filling in missing data with estimated values.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport seaborn as sns\ndf=sns.load_dataset('titanic')\ndf['mean_age']=df['age'].fillna(df['age'].mean())",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ">iii]Forward fill and backward fill: This technique involves propagating the previous or next value to fill in the missing data.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport seaborn as sns\ndf=sns.load_dataset('titanic')\nfrom types import MethodType\ndf['age'].fillna(method='ffill')",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport seaborn as sns\ndf=sns.load_dataset('titanic')\nfrom types import MethodType\ndf['age'].fillna(method='bfill')",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ">iv]Interpolation: This technique involves using the other data points to estimate the missing values.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport seaborn as sns\ndf=sns.load_dataset('titanic')\ndf['age'].interpolate()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Imbalanced data refers to a dataset where the distribution of the target variable is not equal among the classes. In other words, one class may have significantly more or fewer observations than the others. For example, in a binary classification problem, if 90% of the observations belong to class A and only 10% belong to class B, the dataset is imbalanced.\nIf imbalanced data is not handled, it can lead to biased and inaccurate results. This is because most machine learning algorithms are designed to maximize overall accuracy, which can result in the model predicting only the majority class, completely ignoring the minority class. In such cases, the model will have high accuracy but low sensitivity for the minority class.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">Upsampling and downsampling are the two commenly used techniques to handle imbalance data.\nUp-sampling involves randomly duplicating minority class observations to increase their representation in the dataset. This technique can be useful when the minority class has important patterns or features that are not present in the majority class.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.utils import resample\nimport pandas as pd\ndf = pd.DataFrame({'X1': [1,2,3,4,5,6,7,8,9,10], 'X2': [0,0,0,0,0,1,1,1,1,1], 'Y': [0,0,0,0,0,1,1,1,1,1]})\ndf_majority = df[df.Y==0]\ndf_minority = df[df.Y==1]\ndf_minority_upsampled = resample(df_minority, replace=True, n_samples=6, random_state=123)\ndf_upsampled = pd.concat([df_majority, df_minority_upsampled])\nprint(df_upsampled)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "text": "   X1  X2  Y\n0   1   0  0\n1   2   0  0\n2   3   0  0\n3   4   0  0\n4   5   0  0\n7   8   1  1\n9  10   1  1\n7   8   1  1\n6   7   1  1\n8   9   1  1\n7   8   1  1\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": ">Down-sampling, on the other hand, involves randomly removing observations from the majority class to decrease its representation in the dataset. This technique can be useful when the majority class contains noisy or redundant data that can negatively impact the model's performance.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.utils import resample\nimport pandas as pd\ndf = pd.DataFrame({'X1': [1,2,3,4,5,6,7,8,9,10], 'X2': [0,0,0,0,0,1,1,1,1,1], 'Y': [0,0,0,0,0,1,1,1,1,1]})\ndf_majority = df[df.Y==0]\ndf_minority = df[df.Y==1]\ndf_majority_downsampled = resample(df_majority, replace=False, n_samples=2, random_state=123)\ndf_downsampled = pd.concat([df_majority_downsampled, df_minority])\nprint(df_downsampled)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "text": "   X1  X2  Y\n1   2   0  0\n3   4   0  0\n5   6   1  1\n6   7   1  1\n7   8   1  1\n8   9   1  1\n9  10   1  1\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Q5: What is data Augmentation? Explain SMOTE.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">Data augmentation is a technique used to artifically increace the size of a data set by create new sample throught transformation or combination of the orignal data.This technique is used to overcome problem of limited data avaliable,especially in image or text data.\nSmote is the most popular  data augmentation technique used to handle imbalanced data by genrating synthatic sample of the minority class.SMOTE works by creating new samples based on the feature space of the minority class. It selects one or more similar observations from the minority class and creates new synthetic observations by interpolating between the selected observations.\nThe SMOTE algorithm can be described in the following steps:\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">i]For each minority class observation, the k nearest neighbors are identified.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">ii]A synthetic sample is created by randomly selecting one of the k nearest neighbors and using it to create a new observation along the line segment joining the original observation and the selected neighbor.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">iii]This process is repeated for a specified number of new synthetic samples.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">Outliers are data points that are significantly different from other observations in a dataset. They can be caused by measurement errors, data entry errors, or represent true but rare events. Outliers can have a significant impact on statistical analyses and machine learning models, as they can bias estimates of central tendency, spread, and correlations.\nHandling outliers is essential for several reasons:\nOutliers can distort statistical analyses and machine learning models, leading to incorrect conclusions or poor performance. For example, in linear regression, outliers can significantly affect the slope and intercept of the regression line, leading to inaccurate predictions.\nOutliers can also affect the normality assumption of many statistical tests, which can negatively impact the validity of the results.\nOutliers can lead to overfitting in machine learning models, as they can be treated as important features by the model, even though they represent noise or irrelevant information.\nThere are several techniques for handling outliers in a dataset, including:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">Removal: One approach to handling outliers is to remove them from the dataset. However, this approach should be used with caution, as it can lead to a loss of valuable information, especially if the outliers represent important rare events.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">Winsorization: Winsorization involves replacing extreme values with less extreme values, typically by replacing values beyond a certain threshold with the nearest value below or above the threshold.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">Transformation: Transformation involves applying mathematical functions to the data to change its distribution and reduce the impact of outliers. Common transformations include logarithmic, square root, and reciprocal transformations.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q7: You are working on a project that requires analyzing customer data. However, you notice that some ofthe data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">There are various techniques that can be used to handle missing data,some of which are:\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">i]Deletion:If the amount of missing data is small then it may be acceptable to remove observation.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">ii]Imputation:It is technique is used to fill missing data with  mean imputation, median imputation, and regression imputation.  \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">iii]Multiple imputation:It involves creating multiple imputed datasets, each of which contains a plausible set of values for the missing data. The analysis is then performed on each imputed dataset, and the results are combined to obtain an overall estimate.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">iv]Modeling technique:It is maimum likelihood estimation can be used to handle missing data. These techniques allow the analysis to be performed on the available data, while accounting for the missing data.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">Here are some technique to determine pattern in missing data:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">i]Visual inspection: Create a heatmap or missingness plot which shows missing values in dataset.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">ii]Correction analysis: Correlation analysis can help identify whether there is a relationship between the missing values and other variables in the dataset. If there is a pattern to the missing data, it may be related to other variables in the dataset.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">iii]Missingness tests: Missingness tests can be used to determine whether the missing data is missing at random or not.The most commonly used missingness test is the Little's MCAR test, which tests the null hypothesis that the missing data is missing completely at random. If the test rejects the null hypothesis, it suggests that the missing data is not MCAR and that there is a pattern to the missing data.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">iv]Imputation methods: Imputation methods can also be used to identify patterns in the missing data. If a particular imputation method performs better than others, it may indicate that there is a pattern to the missing data that the imputation method is accounting for.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q9: suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">Working with imbalanced datasets is a common challenge in machine learning, especially in medical diagnosis projects. Here are some strategies to evaluate the performance of your machine learning model on an imbalanced dataset:\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">Use appropriate evaluation metrics: Using appropriate evaluation metrics is crucial when dealing with imbalanced datasets. Metrics such as accuracy can be misleading as they may indicate high performance when in fact the model is simply predicting the majority class. Instead, metrics such as precision, recall, F1-score, and area under the receiver operating characteristic curve (AUC-ROC) are more appropriate for imbalanced datasets.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">Use resampling techniques: Resampling techniques such as oversampling the minority class, undersampling the majority class, or generating synthetic samples can help balance the dataset. However, it is important to carefully consider the implications of using these techniques and select the most appropriate one for the specific project.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">Adjust class weights: Many machine learning algorithms allow you to adjust class weights to give more importance to the minority class. This can help the model focus on correctly identifying the positive cases.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">Use ensemble techniques: Ensemble techniques such as bagging or boosting can help improve performance on imbalanced datasets by combining multiple models. These techniques can help reduce the impact of noisy data and improve the generalization of the model.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">Stratified sampling: If you are splitting your dataset into training and testing sets, stratified sampling can help ensure that both sets have a similar distribution of the classes.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">When working with an unbalanced dataset, where one class is overrepresented, it is important to balance the dataset to avoid biased results. Here are some methods to balance the dataset and down-sample the majority class:\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">Random undersampling: Randomly removing instances from the majority class can balance the dataset. This can be done by setting a threshold on the ratio of majority class instances to minority class instances.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">Cluster-based undersampling: Grouping similar instances from the majority class and keeping only representative instances can balance the dataset. This can be done using clustering algorithms like K-means or hierarchical clustering.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">Synthetic Minority Over-sampling Technique (SMOTE): SMOTE generates synthetic samples for the minority class based on the nearest neighbors in the feature space. This technique can balance the dataset and provide more information to the model about the minority class.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">Ensemble techniques: Ensemble techniques like bagging or boosting can also be used to balance the dataset. By combining multiple models, the impact of the majority class can be reduced.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">Stratified sampling: When splitting the dataset into training and testing sets, stratified sampling can help ensure that both sets have a similar distribution of the classes.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q11: You discover that the dataset is unbalanced with a low percentage Of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up—sample the minority class?\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "When working with an unbalanced dataset where one class is underrepresented, it is important to balance the dataset to avoid biased results. Here are some methods to balance the dataset and up-sample the minority class:\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">Random oversampling: Randomly duplicating instances from the minority class can balance the dataset. This can be done by setting a threshold on the ratio of minority class instances to majority class instances.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">Synthetic Minority Over-sampling Technique (SMOTE): SMOTE generates synthetic samples for the minority class based on the nearest neighbors in the feature space. This technique can balance the dataset and provide more information to the model about the minority class.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">Adaptive Synthetic (ADASYN) sampling: ADASYN generates synthetic samples for the minority class based on the density distribution of the feature space. This technique can balance the dataset and provide more information to the model about the minority class.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">Ensemble techniques: Ensemble techniques like bagging or boosting can also be used to balance the dataset. By combining multiple models, the impact of the minority class can be increased.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">Cost-sensitive learning: Cost-sensitive learning is a technique that assigns different costs to misclassifications of different classes. This can help the model focus on correctly identifying the rare event.",
      "metadata": {}
    }
  ]
}