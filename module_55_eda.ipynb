{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Min-Max scaling, also known as normalization, is a technique used in data preprocessing to transform the values of numeric features to a common scale, typically between a specified minimum and maximum value, often 0 and 1 or -1 and 1. This technique helps to eliminate the units of measurement and ensures that features contribute equally to the model.\n",
    "\n",
    "\n",
    ">Formula:\n",
    "X′ =  (X−Xmin)/(Xmax −Xmin)\n",
    "\n",
    ">Where:\n",
    "\n",
    ">𝑋 is the original value,\n",
    "\n",
    ">Xmin  is the minimum value in the dataset,\n",
    "\n",
    ">Xmax is the maximum value in the dataset,\n",
    "\n",
    ">X′ is the scaled value.​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The Unit Vector technique, also known as vector normalization, scales the feature values such that the Euclidean norm (L2 norm) of the vector equals 1. This technique is particularly useful when the direction of the data is more important than its magnitude.\n",
    "\n",
    "> Formula:\n",
    "X′ =  X /||X||2\n",
    "\n",
    ">Where:\n",
    "\n",
    ">𝑋 is the original vector,\n",
    "\n",
    ">∥X∥2  is the Euclidean norm of \n",
    "\n",
    ">X′ is the normalized vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principal Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction by transforming the original features into a new set of uncorrelated features called principal components. These principal components are ordered by the amount of variance they capture from the data. PCA helps to reduce the number of features while retaining as much information as possible.\n",
    "\n",
    "**How it works**:\n",
    "\n",
    "> Standardize the Data: Ensure the data is centered around the mean and has unit variance.\n",
    "\n",
    "> Compute Covariance Matrix: Calculate the covariance matrix to understand how features vary together.\n",
    "\n",
    "> Compute Eigenvalues and Eigenvectors: Find the eigenvalues and eigenvectors of the covariance matrix. Eigenvectors determine the direction of the new feature space, while eigenvalues determine their magnitude.\n",
    "\n",
    "> Sort Eigenvectors: Order the eigenvectors by their corresponding eigenvalues in descending order.\n",
    "\n",
    "> Transform Data: Project the data onto the new feature space defined by the top k eigenvectors (principal components)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Principal Component Analysis (PCA) is a technique that can be used for feature extraction by transforming the original set of features into a new set of features called principal components. These principal components are linear combinations of the original features and are ordered by the amount of variance they capture from the data.\n",
    "\n",
    "> Relationship:\n",
    "\n",
    ">Feature Extraction: PCA reduces the dimensionality of the dataset by identifying the most significant features (principal components) that capture the most information (variance).\n",
    "\n",
    ">Dimensionality Reduction: By selecting the top principal components, PCA allows us to reduce the number of features while retaining the essential information, thus simplifying the dataset.\n",
    "\n",
    "> Example:\n",
    "Consider a dataset with three features: [Height, Weight, Age]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To use Min-Max scaling to preprocess the data for a recommendation system, follow these steps:\n",
    "\n",
    "> Identify Features: Extract the features that need scaling (price, rating, delivery time).\n",
    "\n",
    "> Determine Range: Decide the range for scaling, typically [0, 1].\n",
    "\n",
    "> Apply Min-Max Scaling: For each feature, apply the Min-Max scaling formula.\n",
    "\n",
    "> Example:\n",
    "\n",
    "> Price: [5, 10, 15, 20] ,\n",
    "Rating: [1, 2, 3, 4] ,\n",
    "Delivery Time: [30, 45, 50, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To use PCA to reduce the dimensionality of the dataset for predicting stock prices, follow these steps:\n",
    "\n",
    ">Standardize the Data: Ensure the data is centered and scaled.\n",
    "\n",
    ">Compute Covariance Matrix: Calculate the covariance matrix to understand the variance and covariance of the features.\n",
    "\n",
    ">Compute Eigenvalues and Eigenvectors: Find the eigenvalues and eigenvectors of the covariance matrix. These eigenvectors are the principal components.\n",
    "\n",
    ">Select Principal Components: Choose the top principal components that capture the majority of the variance. Typically, we choose enough components to capture around 95% of the total variance.\n",
    "\n",
    ">Transform Data: Project the original data onto the new feature space defined by the selected principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To perform Min-Max scaling to transform the values to a range of -1 to 1, follow these steps:\n",
    "\n",
    "> 1.Determine Range: Desired range is [-1, 1].\n",
    "\n",
    "> 2.Apply Min-Max Scaling Formula:\n",
    "X′ =2. (X−Xmin ) / (xmax-xmin)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To perform Feature Extraction using PCA and determine the number of principal components to retain, follow these steps:\n",
    "\n",
    "> Standardize the Data: Ensure the features are centered and scaled.\n",
    "\n",
    "> Compute Covariance Matrix: Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "> Compute Eigenvalues and Eigenvectors: Determine the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "> Explained Variance: Calculate the explained variance for each principal component."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
